

8

detector, intervention approach, and results of a summative
evaluation study 1.

1.1 related work
the idea of attention-aware user interfaces is not new, but was
proposed almost a decade ago by roda and thomas [39]. there
was even an article on futuristic applications of attention-aware
systems in educational contexts [35]. prior to this, gluck, et al.
[15] discussed the use of eye tracking to increase the bandwidth
of information available to an intelligent tutoring system (its).
similarly, anderson [1] followed up on some of these ideas by
demonstrating how particular beneficial instructional strategies
could only be launched via a real-time analysis of eye gaze.
most of the recent work has been on leveraging eye gaze to
increase the bandwidth of learner models [22, 23, 29]. conati, et
al. [5] provide an excellent review of much of the existing work
in this area. we can group the research into three categories: (1)
offline-analyses of eye gaze to study attentional processes, (2)
computational modeling of attentional states, and (3) closed-loop
systems that respond to attention in real-time. offline-analysis of
eye movements has received considerable attention in cognitive
and educational psychology for several decades [e.g., 16, 19], so
this area of research is relatively healthy. online computational
models of learner attention are just beginning to emerge [e.g., 6,
11], while closed-loop attention-aware systems are few and far
between (see [7, 15, 42, 48] for a more or less exhaustive list).
two known examples, gazetutor and attentivereview, are
discussed below.
gazetutor [7] is a learning technology for biology. it has an
animated conversational agent that provides spoken explanations
on biology topics which are synchronized with images. the
system uses a tobii t60 eye tracker to detect inattention, which
is assumed to occur when learners’ gaze is not on the tutor agent
or image for at least five consecutive seconds. when this occurs,
the system interrupts its speech mid utterance, directs learners to
reorient their attention (e.g., “i’m over here you know”), and
repeats speaking from the start of the current utterance. in an
evaluation study, 48 learners (undergraduate students) completed
a learning session on four biology topics with the attention-aware
components enabled (experimental group) or disabled (control
group). the results indicated that gazetutor was successful in
dynamically reorienting learners’ attentional patterns towards the
interface. importantly, learning gains for deep reasoning
questions were significantly higher for the experimental vs.
control group, but only for high aptitude learners. the results
suggest that even the most basic attention-aware technology can
be effective in improving learning, at least for a subset of learners.
however, a key limitation is that the researchers simply assumed
that off-screen gaze corresponded to inattention, but did not test
this assumption (e.g., students could have been concentrating
with their eyes closed and this would have been perceived as
being inattentive).
attentivereview [32] is a closed-loop system for mooc learning
on mobile phones. the system uses video-based
photoplethysmography (ppg) to detect a learners’ heart rate from
the back camera of a smartphone while they view mooc-like
lectures on the phone. attentivereview ranks the lectures based
1

on its estimates of learners’ “perceived difficulty,” selecting the
most difficult lecture for subsequent review (called adaptive
review). in a 32-participant between-subjects evaluation study,
the authors found that learning gains obtained from the adaptive
review condition were statistically on par with a full review
condition, but were achieved in 66.7% less review time. although
this result suggests that attentivereview increased learning
efficiency, there is the question as to whether the system should
even be considered to be an “attention-aware” technology. this is
because it is arguable if the system has anything to do with
attention (except for “attention” appearing in its name) as it
selects items for review based on a model of “perceived
difficulty” and not on learners’ “attentional state.” the two might
be related, but are clearly not the same.

1.2 novelty
our paper focuses on closing the loop between research on
educational data and learning outcomes by developing and
validating the first (in our view) real-time learning technology
that detects and mitigates mind wandering during computerized
reading. although automated detection of complex mental states
with the goal of developing intelligent learning technologies that
respond to the sensed states is an active research area (see reviews
by [9, 18]), mind wandering has rarely been explored as an aspect
of a learner’s mental state that warrants detection and corrective
action. and while there has been some work on modeling the
locus of learner attention (see review by [5]), mind wandering is
inherently different than more commonly studied forms of
attention (e.g., selective attention, distraction), because it involves
more covert forms of involuntary attentional lapses spawned by
self-generated internal thought [45]. simply put, mind wandering
is a form of “looking without seeing” because the eyes might be
fixated on the appropriate external stimulus, but very little is
being processed as the mind is consumed by stimulusindependent internal thoughts. offline automated approaches to
detect mind wandering have been developed (e.g., [3, 11, 27, 33]),
but these detectors have not yet been used to trigger online
interventions. here, we adapt an offline gaze-based automated
mind wandering detector [13] to trigger real-time interventions to
address mind wandering during reading. we conduct a
randomized control trial to evaluate the efficacy of our attentionaware learning technology in improving learning.

2. mind wandering detection
we adopted a supervised learning approach for mind wandering
detection. below we provide a high-level overview of the
approach; readers are directed to [3, 13] for a detailed discussion
of the general approach used to build gaze-based detectors of
mind wandering.

2.1 training data
we obtained training data from a previous study [26] that
involved 98 undergraduate students reading a 57-page text on the
surface tension of liquids [4] on a computer screen for an average
of 28 minutes. the text contained around 6500 words, with an
average of 115 words per page, and was displayed on a computer
screen with courier new typeface. we recorded eye-gaze with a
tobii tx300 eye tracker set to a sampling frequency of 120 hz.

this paper reports updated results of an earlier version [10] presented
as a “late-breaking work” (lbw) poster at the 2016 acm chi
conference. lbw “extended abstracts” are not included in the main
conference proceedings and copyright is retained by the authors.



9

participants could read normally and were free to move or gesture
as they pleased.
participants were instructed to report mind wandering (during
reading) by pressing a predetermined key when they found
themselves “thinking about the task itself but not the actual
content of the text” or when they were “thinking about anything
else besides the task.” this is consistent with contemporary
approaches (see [45]) that rely on self-reporting because mind
wandering is an internal conscious phenomena. further, selfreports of mind wandering have been linked to predictable
patterns in physiology [43], pupillometry [14], eye-gaze [37], and
task performance [34], providing validity for this approach.

on the page, we classified the page as a positive instance of mind
wandering. this was done because analyses indicated that
participants were more likely to be mind wandering in those cases
(but see [13] for alternate strategies to handle missing instances).

on average, we received mind wandering reports for 32% of the
pages (sd = 20%), although there was considerable variability
among participants (ranging from 0% to 82%). self-reported
mind wandering negatively correlated (r = -.23, p < .05) with
scores on a subsequent comprehension assessment [26], which
provides evidence for the predictive validity of the self-reports.

2.2 model building
the stream of eye-gaze data was filtered to produce a series of
fixations, saccades, and blinks, from which global eye gaze
features were extracted (see figure 1). global features are
independent of the words being read and are therefore more
generalizable than so-called local features. a full list of 62 global
features along with detailed descriptions is provided in [13], but
briefly the features can be grouped into the following four
categories: (1) eye movement descriptive features (n = 48) were
statistical functionals (e.g., min, median) for fixation duration,
saccade duration, saccade amplitude, saccade velocity, and
relative and absolute saccade angle distributions; (2) pupil
diameter descriptive features were statistical functionals (n = 8)
computed from participant-level z-score standardized estimates
of pupil diameter; (3) blink features (n = 2) consisted of the
number of blinks and the mean blink duration; (4) miscellaneous
gaze features (n = 4) consisted of the number of saccades,
horizontal saccade proportion, fixation dispersion, and the
fixation duration/saccade duration ratio. we proceeded with a
subset of 32 features after eliminating features exhibiting
multicollinearity.
features were calculated from only a certain amount of gaze data
from each page, called the window. the end of the window was
positioned 3 seconds before a self-report so as to not overlap with
the key-press. the average amount of time between self-reports
and the beginning of the page was 16 seconds. we used this time
point as the end of the window for pages with no self-report.
pages that were shorter than the target window size were
discarded, as were pages with windows that contained fewer than
five gaze fixations as there was insufficient data to compute some
of the features. there were a total of 4,225 windows with
sufficient data for supervised classification.
we experimented with a number of supervised classifiers on
window sizes of 4, 8, and 12 seconds to discriminate positive
(pages with a self-report = 32%) from negative (pages without a
self-report) instances of mind wandering. the training data were
downsampled to achieve a 50% base rate; testing data were
unaltered. a leave-one-participant-out validation approach was
adopted where models were built on data from n-1 participants
and evaluated on the held-out participant. the process was
repeated for all participants. model validation was conducted in a
way to simulate a real-time system by analyzing data from every
page. when classification was not possible due to a lack of valid
gaze data and/or because participants did not spend enough time

figure 1: gaze fixations during mind wandering (top)
and normal reading (bottom)

2.3 detector accuracy
the best model was a support vector machine that used global
features and operated on a window size of 8-seconds. the area
under the roc curve (auc or auroc or a’) was .66, which
exceeds the 0.5 chance threshold [17].
we assigned each instance as mind wandering or not mind
wandering based on whether the detector’s predicted likelihood
of mind wandering (ranges from 0 to 1) was below or above 0.5
we adopted the default 0.5 threshold as it led to a higher rate of
true positives while maintaining a moderate rate of true negatives.
this resulted in the following confusion matrix shown in table 1.
the model had a weighted precision of 72.2% and a weighted
recall of 67.4%, which we deemed to be sufficiently accurate for
intervention.
table 1: proportionalized confusion matrix for mind
wandering detection
predicted mind wandering (mw)
actual mw

yes

no

yes

0.715 (hit)

0.285 (miss)

no

0.346 (false positive)

0.654 (correct rejection)

3. intervention to address mind wandering
our intervention approach is grounded in the basic idea that
learning of conceptual information involves creating and
maintaining an internal model (mental model) by integrating
information from the text with prior knowledge from memory
[25]. this integration process relies on attentional focus and
breaks down during mind wandering because information from
the external environment is no longer being integrated into the
internal mental model. this results in an impaired model which
leads to less effective suppression of off-task thoughts. this
increase in mind wandering further impairs the mental model,



10

resulting in a vicious cycle. our intervention targets this vicious
cycle by redirecting attention to the primary task and attempting
to correct for comprehension deficits attributed to mind
wandering. based on research demonstrating the effectiveness of
interpolated testing [47], we propose that asking questions on
pages where mind wandering is detected and encouraging rereading in response to incorrect responses will aid in re-directing
attention to the text and correct knowledge deficits.

page regardless of whether the second question was answered
correctly, so as not to be overly burdensome.

3.1 intervention implementation
our initial intervention was implemented for the same text used
to create the mind wandering detector (although it could be
applied to any text). the text was integrated into the computer
reading interface. mind wandering detection occurred when the
learner navigated to the next page using the right arrow key. in
order to address ambiguity in mind wandering detection, we used
the detector’s mind wandering likelihood to probabilistically
determine when to intervene. for example, if the mind wandering
likelihood was 70%, then there was a 70% chance of intervention
on any given page (all else being equal). we did not intervene for
the first three pages in order to allow the learner to become
familiar with the text and interface. to reduce disruption, there
was a 50% reduced probability of intervening on adjacent pages,
and the maximum number of interventions was capped at 1/3 ×
the number of pages (19 for the present 57-page text). table 2
presents pseudo code for when to launch an intervention.
table 2: pseudo code for intervention strategy
launch_intervention:
if current_page >= waitpages
and
total_interventions < maxintrv)
and
gaze_likelihood > random(0,1)
and
(!has_intervened(previous_page)
or 0.5 < random (0,1)):
do_intervention()
else:
show_next_page()
do_intervention:
answer1 = show_question1()
if answer1 is correct:
show_positive_feedback()
show_next_page()
else:
show_neg_feedback()
suggest_rereading()
if page advance detected:
answer2 = show_question2();
show_next_page()

figure 2 presents an outline of the intervention strategy. the
intervention itself relied on two multiple choice questions for
each page (screen) of the text. when the system decided to
intervene, one of the questions (randomly selected) was presented
to the learner. if the learner answered this online question
correctly, positive feedback was provided, and the learner could
advance to the next page. if the learner answered incorrectly,
negative feedback was provided, and the system encouraged the
learner to re-read the page. the learner was then provided with a
second (randomly selected) online question, which could either
be the same or the alternate question for that page. feedback was
not provided and the learner was allowed to advance to the next

figure 2: outline of intervention strategy

3.2 iterative refinement
the technology was refined through multiple rounds of formative
testing with 67 participants, recruited from the same institution
used to build the detector. participants were observed while
interacting with the technology, their responses were analyzed,
and they were interviewed about their experience. we used the
feedback gleaned from these tests to refine the intervention
parameters (i.e., when to launch, how many interventions to
launch, whether to launch interventions on subsequent pages),
intervention questions themselves, and instructions on how to
attend to the intervention. for example, earlier versions of the
intervention used a fixed threshold (instead of the aforementioned
probabilistic approach) to trigger an intervention. despite many
attempts to set this threshold, the end result was that some
participants received many interventions while others received
almost no interventions. this issue was corrected by
probabilistically rather than deterministically launching the
intervention. additional testing/refinement of the comprehension
questions used in the intervention was done using crowdsourcing
platforms, specifically amazon’s mechanical turk (mturk).

4. evaluation study
we conducted a randomized controlled trial to evaluate the
technology. the experiment had two conditions: an intervention
condition and a yoked control condition (as described below). the
yoked control was needed to verify that any learning benefits are
attributed to the technology being sensitive to mind wandering
and not merely to the added opportunities to answer online
questions and re-read. this is because we know that interpolated
testing itself has beneficial comprehension effects [47].

4.1 method
participants (n = 104) were a new set of undergraduate students
who participated to fulfill research credit requirements. they
were recruited from the same university used to build the mw
detector and for the iterative testing and refinement cycles.
we did not use a pretest because we expected participants to be
unfamiliar with the topic. participants were not informed that the
interface would be tracking their mind wandering (until the



11

debriefing at the end), instead, they were instructed as follows:
“while reading the text, you will occasionally be asked some
questions about the page you just read. depending on your
answer, you will re-read the same page and you will be asked
another question that may or may not be the same question.”
participants in the intervention condition received the
intervention as described above (i.e., based on detected mind
wandering likelihoods). each participant in the yoked control
condition was paired with a participant in the intervention
condition. he or she received an intervention question on the
same pages as their paired intervention participant regardless of
mind wandering likelihood. for example, if participant a (i.e.,
intervention condition) received questions on pages 5, 7, 10, and
25, participant b (i.e., yoked control condition) would receive
intervention questions on the same pages. however, if the yoked
participant answered incorrectly, then (s)he had the opportunity
to re-read and answer another question regardless of the outcome
of their intervention-condition partner.
after reading, participants completed a 38-item multiple choice
comprehension assessment to measure learning. the questions
were randomly selected from the 57 pages (one per page) with the
exception that a higher selection priority was given to pages that
were re-read on account of the intervention. participants in the
yoked control condition received the same posttest questions as
their intervention condition counterparts.

4.2 results
participants received an average of 16 (min of 7 and max of 19)
interventions. they spent an average of 27.5 seconds on each
screen prior to receiving an intervention. there was no significant
difference across conditions (p = .998), suggesting that reading
time was not a confound. in what follows, we compared each
intervention participant to his/her yoked control with a two-tailed
paired-samples t-test and a 0.05 criteria for statistical
significance.
mind wandering detection. the detector’s likelihood of mind
wandering was slightly higher for participants in the yokedcontrol condition (m = .431; sd = .170) compared to the
intervention condition (m = .404; sd = .112), but the difference
was not statistically significant (p = .348). this was unsurprising
as participants in both groups received the same interventions,
which itself was expected to reduce mind wandering. importantly,
mind wandering likelihoods were negatively correlated with
performance on the online questions (r = -.296, p = .033) as well
as on posttest questions (r = -.319, p = .021). this provides
evidence for the validity of the mind wandering detector when
applied to a new set of learners and under different conditions
(i.e., reading interspersed with online questions compared to
uninterrupted reading).
comprehension assessment. there was some overlap between
the online questions and the posttest questions. to obtain an
unbiased estimate of learning, we only analyzed performance on
previously unseen posttest questions. that is, questions that were
used as part of the intervention were first removed before
computing posttest scores.
there were no significant condition differences on overall
posttest scores (p = .846). the intervention condition answered
57.6% (sd = .157) of the questions correctly while the yoked
control condition answered 58.1% (sd = .129) correctly. this
finding was not surprising as both conditions received the exact
same treatment except that the interventions were triggered based

on detected mind wandering in the intervention condition but not
the control condition.
next, we examined posttest performance as a function of mind
wandering during reading. each page was designated as a low or
high mind wandering page based on a median split of mind
wandering likelihoods (medians = .35 and .36 on a 0 to 1 scale for
intervention and control conditions, respectively). we then
analyzed performance on posttest questions corresponding to
pages with low vs. high likelihoods of mind wandering (during
reading). the results are shown in table 3.
we found no significant posttest differences on pages where both
the intervention and control participants had low (p = .759) or
high (p = .922) mind wandering likelihoods (first and last rows in
table 3, respectively). there was also no significant posttest
difference (p = .630) for pages where the intervention condition
had high mind wandering likelihoods but the control condition
had low mind wandering likelihoods (row 3). however, the
intervention condition significantly (p = .003, d = .47 sigma)
outperformed the control condition for pages where the
intervention participants had low likelihoods of mind wandering
but control participants had high mind wandering likelihoods
(row 2). these last two finding suggests that the intervention had
the intended effect of reducing comprehension deficits
attributable to mind wandering because it led to equitable
performance when mind wandering was high and improved
performance when it was low.
table 3: posttest performance (proportion of correct
responses) as a function of mind wandering during reading.
standard deviations in parenthesis.
mind
wandering

posttest

n

int.

cntrl.

int.

cntrl.

43

low

low

.604 (.288)

.623 (.287)

40

low

high

.643 (.263)

.489 (.298)

43

high

low

.535 (.295)

.566 (.305)

45

high

high

.522 (.312)

.515 (.291)

scores

note. int. = intervention. cntrl. = control. bolded cells represent a
statistically significant difference. n = number of pairs (out of 52) in each
analysis. it differs slightly across analyses as not all participants were
assigned to each mind wandering group.

after-task interview. we interviewed a subset of the participants
in order to gauge their subjective experience with the
intervention. a few key themes emerged. participants reported
paying closer attention to the text after realizing they would be
periodically answering multiple-choice questions. this was good.
however, participants also reported that they adapted their
reading strategies in one of two ways in response to the questions.
since the questions targeted factual information (sometimes
verbatim) from the text, some participants paid more attention to
details and precise wordings instead of the broader concepts being
discussed in the text. more discouragingly, some participants
reported adopting a preemptive skimming strategy in that they
would only look for keywords that they expected to appear in a
subsequent question.
participants were encouraged to re-read text when they answered
incorrectly before receiving another question (or the same
question in some cases). many participants reported simply
scanning the text (when re-reading) to locate keywords from the
question before moving on. since the scanning strategy was often



12

successful to answer the subsequent question, participants
reported that the questions were too easy and it took relatively
little effort to locate the correct answer compared to re-reading.
they suggested that it may have been better if the questions had
targeted key concepts rather than facts.
finally, participants reported difficulties with re-engaging with
the text after answering an online question because the text was
cleared when an intervention question was displayed; an item that
can be easily corrected in subsequent versions.

5. discussion
we developed the first educational technology capable of realtime mind wandering detection and dynamic intervention during
computerized reading. in the remainder of this section, we discuss
the significance of our main findings, limitations, and avenues for
future work.

5.1 significance of main findings
we have three main findings. first, we demonstrated that a
machine-learned mind wandering detector built in one context
can be applied to a different (albeit related) interaction context.
specifically, the detector was trained on a data set involving
participants silently reading and self-reporting mind wandering,
but was applied to an interactive context involving interpolated
assessments, which engendered different reading strategies.
further, self-reports of mind wandering were not collected in this
interactive context, which might have influenced mind wandering
rates in and of itself. despite these differences, we were able to
demonstrate the predictive validity of the detector by showing
that it negatively correlated with both online and offline
comprehension scores when evaluated on new participants.
second, we showed promising effects for our intervention
approach despite a very conservative experimental design, which
ensured that the intervention and control groups were equated
along all respects, except that the intervention was triggered based
on the mind wandering detector (key manipulation). further, we
used a probabilistic approach to trigger an intervention, because
the detector is inherently imperfect. as a result, participants could
have received an intervention when they were not mind
wandering and/or could have failed to receive one when they were
mind wandering. therefore, it was essential to compare the two
groups under conditions when the mind wandering levels
differed. this more nuanced analysis revealed that although the
intervention itself did not lead to a boost in overall comprehension
(because it is remedial), it equated comprehension scores when
mind wandering was high (i.e., scores for the intervention group
were comparable when the control group was low on mind
wandering). it also demonstrated the cost of not intervening
during mind wandering (i.e., scores for the intervention group
were greater when the control group was high on mind
wandering). in other words, the intervention was successful in
mitigating the negative effects of mind wandering.
third, despite the advantages articulated above, the intervention
itself was reactive and engendered several unintended (and
presumably suboptimal) behaviors. in particular, students altered
their reading strategies in response to the interpolated questions,
which were a critical part of the intervention. in a sense, they
attempted to “game the intervention” by attempting to proactively
predict the types of questions they might receive and then
adopting a complementary reading strategy consisting of
skimming and/or focusing on factual information. this reliance
on surface- rather than deeper-levels of processing was
incongruent with our goal of promoting deep comprehension.

5.2 limitations
there are a number of methodological limitations with this work
that go beyond limitations with the intervention (as discussed
above). first, we focused on a single text that is perceived as
being quite dull and consequently triggers rather high levels of
mind wandering [26]. this raises the question of whether the
detector will generalize to different texts. we expect some level
of generalizability in terms of features used because the detector
only used content- and position- (on the screen) free global gaze
features. however, given that several supervised classifiers are
very sensitive to differences in base rates, the detector might overor under- predict mind wandering when applied to texts that
engender different rates of mind wandering. therefore, retraining
the detector with a more diverse set of texts is warranted.
another limitation is the scalability of our learning technology.
the eye tracker we used was a cost-prohibitive tobii tx300 that
will not scale beyond the laboratory. fortunately, commercialoff-the-shelf (cots) eye trackers, such as eye tribe and tobii
eyex, can be used to surpass this limitation. it is an open question
as to whether the mind wandering detector can operate with
similar fidelity with these cots eye trackers. our use of global
gaze features which do not require high-precision eye tracking
holds considerable promise in this regard. nevertheless,
replication with scalable eye trackers and/or scalable alternatives
to eye tracking (e.g., facial-feature tracking [46] or monitoring
reading patterns [27]) is an important next step (see section 5.3).
our use of surface-level questions for both the intervention and
the subsequent comprehension assessment is also a limitation as
is the lack of a delayed comprehension assessment. it might be
the case that the intervention effects manifest as richer encodings
in long-term memory, a possibility that cannot be addressed in the
current experiment that only assessed immediate learning.
other limitations include a limited student sample (i.e.
undergraduates from a private midwestern college) and a
laboratory setup. it is possible that the results would not
generalize to a more diverse student population or in more
ecological environments (but see below for evidence of
generalizability of the detector in classroom environments).
replication with data from more diverse populations and
environments would be a necessary next step to increase the
ecological validity of this work.

5.3 future work
our future work is progressing along two main fronts. one is to
address limitations in the intervention and design of the
experimental evaluation as discussed above. accordingly, we are
exploring alternative intervention strategies, such as: (a) tagging
items for future re-study rather than interrupting participants
during reading; (b) highlighting specific portions of the text as an
overt cue to facilitate comprehension of critical information; (c)
asking fewer intervention questions, but selecting inference
questions that target deeper levels of comprehension and that span
multiple pages of the text; and (d) asking learners to engage in
reflection by providing written self-explanations of the textual
content. we are currently evaluating one such redesigned
intervention – open-ended questions targeting deeper levels of
comprehension (item c). our revised experimental design taps
both surface- and inference-level comprehension and assesses
comprehension immediately after reading (to measure learning)
and after a one-week delay (to measure retention).
we are also developing attention-aware versions of more
interactive interfaces, such as learning with an intelligent tutoring

